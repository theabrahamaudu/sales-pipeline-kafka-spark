Real-Time Retail Store Sales Pipeline
==============================

Real-Time Retail Store Sales Pipeline with live visualization Dashboard

Project Organization
------------

    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”‚
    â”œâ”€â”€ config             <- Configuration and environment variable files
    â”‚
    â”œâ”€â”€ data               <- Raw data to be streamed
    â”‚
    â”œâ”€â”€ databases          <- Scripts for connecting and setting up databases and tables
    â”‚
    â”œâ”€â”€ docs               <- Documentation files
    â”‚
    â”œâ”€â”€ producer           <- Kafka producer scripts
    â”‚
    â”œâ”€â”€ streaming          <- Spark streaming scripts
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the environment
    â””â”€â”€ setup.py           <- makes project pip installable (pip install -e .) so local modules can be imported

--------
This repository provides scripts for simulating a real-time stream from a local CSV file using a Kafka producer. The stream is subscribed and processed using Spark structured streaming. The raw stream data is stored in a Cassandra table, undergoes transformation, and is saved in a MySQL table. This transformed data can be accessed and visualized in real-time on a Superset Dashboard.

## Dataset Overview
The dataset used in this system consists of three CSV files, of which two of these are actively used:
- `orders.csv`: 
    - This is iterated over and fed to the topic of the Kafka producer
- `customers.csv`:
    - This is loaded to the Hadoop DFS where it can be accessed by Spark

## System Architecture
This system seamlessly streams sales orders from a local file to a Kafka topic. Spark Streaming ingests the stream and takes charge of processing this data, initially storing the raw stream in a Cassandra database. It further enhances the data by combining order and customer information to create a structured dataset, which is subsequently saved in a MySQL table. The MySQL database powers the creation of a live dashboard on Superset, offering real-time insights. 

Here's an overview of the system's architecture:

![Architecture Diagram](https://github.com/theabrahamaudu/sales-pipeline-kafka-spark/raw/main/docs/Sales%20Pipeline.png)

### Kafka Producer
The `orders.csv` file from local storage is used to simulate a continuous stream of data representing orders from the retail store's online platform. In a real-world scenario, this data would typically be generated by a webhook triggered when a customer places an order on the store's website. This setup allows for testing and development of stream processing pipelines in a controlled environment before deploying them to handle live data streams.

### Spark Structured Streaming
Spark Streaming is used to consume data from the Kafka topic and execute transformation and write operations to both a Cassandra and MySQL database. Initially, the raw stream is stored in a Cassandra database for archival purposes. Subsequently, the order data is enriched by aggregating it with customer data retrieved from HDFS. This aggregation process creates a new data structure that facilitates visualization and tracking of key performance indicators (KPIs). In a production environment, the customer data would typically be transformed into a DataFrame from a database using the appropriate connector.

### Real-time Dashboard
Superset is employed for visualizing the aggregated data, allowing users to monitor and track desired metrics and key performance indicators (KPIs). This is accomplished by establishing a connection to the MySQL database and fetching data from the dedicated table where it is stored. To maintain real-time data updates, the data state is periodically refreshed, ensuring that the dashboard reflects the most current information.

## Deployment
This project has been developed and successfully deployed on a local Ubuntu 22.04 environment. It can be seamlessly deployed to a cloud platform by setting up the necessary dependencies and replicating the codebase, configured in accordance with your cloud setup, to a cloud-hosted Ubuntu virtual machine (VM).


--------

## Getting Started
To watch the codebase walkthrough and demo session for the entire project, check out the YouTube video for this project <a href="https://www.youtube.com/@DataCodePy">here</a> [coming soon].

--------
## Experimenting 
### Requirements
- Ubuntu [Tested on Ubuntu 22.04]
- Python 3.7
- Java 8 [Open JDK 1.8]
- Hadoop 3.3.6
- Spark 3.3.3
- Kafka 3.5.1
- Cassandra 4.1.3
- MySQL 8.0.34
- Superset

#### Spark Dependencies [jars]:  
Used in `streamHandler.py` module
```
jsr166e-1.1.0.jar
spark-cassandra-connector-assembly_2.12-3.3.0.jar
mysql-connector-java-8.0.30.jar
spark-sql-kafka-0-10_2.12-3.3.0.jar
kafka-clients-3.5.1.jar
spark-streaming-kafka-0-10-assembly_2.12-3.3.3.jar
commons-pool2-2.11.1.jar
```

### Reproduce Environment
- Install and configure required software as defined in `Requirements` above

- Download Spark dependencies as defined above

- Clone repository:  
    ```
    git clone https://github.com/theabrahamaudu/sales-pipeline-kafka-spark.git
    ```  

- Create Python virtual environment
    ```
    python3 -m venv .venv
    ```

- Install requirements:  
    ```
    pip install -r requirements.txt
    ```
- Edit jars dir in `config.yaml` in the config directory to where you have stored the jar dependencies:
    ```
    jars:
      dir: /path/to/your/jars/lib
    ```

- Create `.env` file in the config directory and add your database login details:
    ```
    MYSQL_USERNAME = 'username'
    MYSQL_PASSWORD = 'password'
    ```

### Reproduce Demo
#### Start Services
- Start Hadoop cluster
- Start Spark cluster

#### Setup Databases and Tables
- Navigate to the databases directory
- Run all cells in `cassandra_script.ipynb`
- Run all cells in `mysql_script.ipynb`

    ###### P.S: These scripts are just to make the steps easier. You can manually follow the steps in the scripts if running on cloud VM makes it tricky to run Jupyter Notebooks.


#### Start Kafka Producer
- Make topic creation bash script `create-topic.sh` executable:
    ```
    chmod +x ./producer/create-topic.sh
    ```
- Create topic:
    ```
    ./producer/create-topic.sh
    ```
- Start producer:
    ```
    python3 ./producer/kafkaProducer.py
    ```

#### Start Spark Streaming
- Copy `customers.csv` from local file system to HDFS:
    ```
    hdfs dfs -put ./data/customers.csv /your/hdfs/directory/customers.csv
    ```

- Set the `customer_data_path` in the object instantiation of the `streamHandler` in `streamScript.py` to "`/your/hdfs/directory/customers.csv`"
    ```
    # Initialize stream handler
    stream = streamHandler(config_path='./config', customer_data_path='/your/hdfs/directory/customers.csv')
    ```

- Run `streamScript.py`
    ```
    python3 ./streaming/streamScript.py
    ```

#### Setup Superset Dashboard
- Start Superset
- Connect to MySQL database
- Create chart using `total_sales_by_source_state` table
- Add the chart to a Dashboard
- Set the refresh interval as desired

*Congratulations! ðŸŽˆ* You have now successfully recreated the Real-Time Retail Store Sales Pipeline with live visualization Dashboard.

## Help

Feel free to reach out to me on any of my socials below if you have any issues experimenting with the project.

## Possible Improvements/Ideas

- [ ] Deployment to cloud
- [ ] Use secondary databases instead of csv files
- [ ] Mock webhook/API architecture for Kafka producer

## Authors

Contributors names and contact info

*Abraham Audu*

* GitHub - [@the_abrahamaudu](https://github.com/theabrahamaudu)
* X (formerly Twitter) - [@the_abrahamaudu](https://x.com/the_abrahamaudu)
* LinkedIn - [@theabrahamaudu](https://www.linkedin.com/in/theabrahamaudu/)
* Instagram - [@the_abrahamaudu](https://www.instagram.com/the_abrahamaudu/)
* YouTube - [@DataCodePy](https://www.youtube.com/@DataCodePy)

## Version History

* See [commit change](https://github.com/theabrahamaudu/sales-pipeline-kafka-spark/commits/main)
* See [release history](https://github.com/theabrahamaudu/sales-pipeline-kafka-spark/releases)

## Acknowledgments

* This project was inspired by DataMaking channel on YouTube
* This [playlist](https://www.youtube.com/playlist?list=PLe1T0uBrDrfOYE8OwQvooPjmnP1zY3wFe) gave me a lot of guidance in building the pipeline
* I got a lot of help from different websites in debugging at different stages (StackOverflow, etc)
